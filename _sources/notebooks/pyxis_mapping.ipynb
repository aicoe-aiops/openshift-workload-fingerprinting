{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585446c2-a44b-420d-bcdd-efe01bb0d8fc",
   "metadata": {},
   "source": [
    "# Mapping Image SHA to Image Name using Pyxis database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce4140-ae3e-4f00-a615-8da273b582b4",
   "metadata": {},
   "source": [
    "In order to answer most of the customer usage and business insights related questions raised in the OpenShift Workload Fingerprinting project, we need to connect two disparate datasets - the insights operator archive and the pyxis database. That is, we want to use pyxis to determine the product name, architecture, vulnerabilities, vendor, etc corresponding to the container image SHA’s in the insights dataset. In a previous [issue](https://github.com/aicoe-aiops/openshift-workload-fingerprinting/issues/11#issuecomment-867776402), we figured out how to do this for a given SHA, by using curl in the terminal. In this notebook, we will try to do this programmatically, and do it for all the SHA’s available in our dataset. We will then store this merged dataset to an s3 bucket and use it for the rest of the analysis in the project going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a9d03-27a4-48a4-856f-cbf18e225e75",
   "metadata": {},
   "source": [
    "## Pre-requisite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcf042-0ec8-4367-963b-c3a9c0dd2d76",
   "metadata": {},
   "source": [
    "In order to fetch the image name (and other details) for the given 'sha' of the image_id, please complete the pre-requisite described below.\n",
    "\n",
    "1. Follow the [link](https://source.redhat.com/groups/public/ccs-onboarding-program/ccs_onboarding_wiki/setting_up_a_kerberos_ticket_and_red_hat_idm) in order to set-up a kerberos ticket and Red Hat IdM on your machine.\n",
    "\n",
    "2. Update the `/etc/krb5.conf` on your machine by setting `dns_canonical_hostname` to `false`, as described in the first 'red box' in this [guide](https://docs.engineering.redhat.com/display/HSSP/Pyxis+access+request)\n",
    "\n",
    "3. Obtain the kerberos ticket by running, `$ kinit <your_kerberos_username>@IPA.REDHAT.COM`\n",
    "\n",
    "\n",
    "In this notebook, we map the given \"sha\" of the image_id for the image layer dataset and container dataset provided from the workload data of the insight operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64149dfa-bf26-46b6-9d8b-cfe9a08feb2e",
   "metadata": {},
   "source": [
    "## Importing useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c730123-f2b5-4990-8fcf-cf3053e2e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from requests_kerberos import HTTPKerberosAuth, OPTIONAL\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ce75ce-a231-46df-9381-8098f906262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6ef55-35c6-4896-af79-d217d921ecab",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "In this section, we will fetch from our s3 bucket the containers dataset and the image layers dataset that have been curated from insights operator archives. To learn more about the general content of datasets, please check out the [getting_started](https://github.com/aicoe-aiops/openshift-workload-fingerprinting/blob/master/notebooks/getting_started_notebook.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d5cb44-ddf7-4b51-9b95-f9438334663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEPH Bucket variables\n",
    "s3_endpoint_url = os.getenv(\"S3_ENDPOINT\")\n",
    "s3_access_key = os.getenv(\"S3_ACCESS_KEY\")\n",
    "s3_secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "s3_secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "s3_bucket = os.getenv(\"S3_BUCKET\")\n",
    "\n",
    "# s3 resource to communicate with storage\n",
    "s3 = boto3.resource(\n",
    "    \"s3\",\n",
    "    endpoint_url=s3_endpoint_url,\n",
    "    aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key,\n",
    ")\n",
    "\n",
    "dates = [\n",
    "    \"2021-08-05\",\n",
    "    \"2021-08-08\",\n",
    "    \"2021-08-10\",\n",
    "    \"2021-08-11\",\n",
    "    \"2021-08-15\",\n",
    "    \"2021-08-17\",\n",
    "    \"2021-08-19\",\n",
    "    \"2021-08-22\",\n",
    "    \"2021-08-23\",\n",
    "]\n",
    "\n",
    "image_layers_df = pd.DataFrame([])\n",
    "image1 = []\n",
    "for date in dates:\n",
    "    obj1 = s3.Object(\n",
    "        s3_bucket,\n",
    "        \"prototype/workload/image_layers/date={date}/{date}.parquet\".format(date=date),\n",
    "    )\n",
    "    buffer1 = io.BytesIO()\n",
    "    obj1.download_fileobj(buffer1)\n",
    "    image1 = pd.read_parquet(buffer1)\n",
    "    image_layers_df = image_layers_df.append(image1)\n",
    "\n",
    "containers_df = pd.DataFrame([])\n",
    "image2 = []\n",
    "for date in dates:\n",
    "    obj2 = s3.Object(\n",
    "        s3_bucket,\n",
    "        \"prototype/workload/containers/date={date}/{date}.parquet\".format(date=date),\n",
    "    )\n",
    "    buffer2 = io.BytesIO()\n",
    "    obj2.download_fileobj(buffer2)\n",
    "    image2 = pd.read_parquet(buffer2)\n",
    "    containers_df = containers_df.append(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74253b82-9432-4a69-ba7f-c5fba9a85492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>layer_image_id</th>\n",
       "      <th>layer_image_level</th>\n",
       "      <th>first_command</th>\n",
       "      <th>first_arg</th>\n",
       "      <th>archive_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e97b920-2876-4076-8bfb-36fe123bc273</td>\n",
       "      <td>sha256:3bc831c3d6614afcd5a8e1728b8bbe6709c957d...</td>\n",
       "      <td>sha256:1cadda38f72dece653de82063e3c8e910265fe7...</td>\n",
       "      <td>0</td>\n",
       "      <td>U7Yi5SISAtKW</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>archives/compressed/9e/9e97b920-2876-4076-8bfb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9e97b920-2876-4076-8bfb-36fe123bc273</td>\n",
       "      <td>sha256:3bc831c3d6614afcd5a8e1728b8bbe6709c957d...</td>\n",
       "      <td>sha256:a50df8fd88fecefc26fd331f832672108deb08c...</td>\n",
       "      <td>1</td>\n",
       "      <td>U7Yi5SISAtKW</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>archives/compressed/9e/9e97b920-2876-4076-8bfb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9e97b920-2876-4076-8bfb-36fe123bc273</td>\n",
       "      <td>sha256:3bc831c3d6614afcd5a8e1728b8bbe6709c957d...</td>\n",
       "      <td>sha256:904d3325f999f09cad1ba9676937fc8b72ff285...</td>\n",
       "      <td>2</td>\n",
       "      <td>U7Yi5SISAtKW</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>archives/compressed/9e/9e97b920-2876-4076-8bfb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             cluster_id  \\\n",
       "0  9e97b920-2876-4076-8bfb-36fe123bc273   \n",
       "1  9e97b920-2876-4076-8bfb-36fe123bc273   \n",
       "2  9e97b920-2876-4076-8bfb-36fe123bc273   \n",
       "\n",
       "                                            image_id  \\\n",
       "0  sha256:3bc831c3d6614afcd5a8e1728b8bbe6709c957d...   \n",
       "1  sha256:3bc831c3d6614afcd5a8e1728b8bbe6709c957d...   \n",
       "2  sha256:3bc831c3d6614afcd5a8e1728b8bbe6709c957d...   \n",
       "\n",
       "                                      layer_image_id  layer_image_level  \\\n",
       "0  sha256:1cadda38f72dece653de82063e3c8e910265fe7...                  0   \n",
       "1  sha256:a50df8fd88fecefc26fd331f832672108deb08c...                  1   \n",
       "2  sha256:904d3325f999f09cad1ba9676937fc8b72ff285...                  2   \n",
       "\n",
       "  first_command first_arg                                       archive_path  \n",
       "0  U7Yi5SISAtKW      <NA>  archives/compressed/9e/9e97b920-2876-4076-8bfb...  \n",
       "1  U7Yi5SISAtKW      <NA>  archives/compressed/9e/9e97b920-2876-4076-8bfb...  \n",
       "2  U7Yi5SISAtKW      <NA>  archives/compressed/9e/9e97b920-2876-4076-8bfb...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_layers_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30309731-d529-4a46-b5ff-f575e17107fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>namespace</th>\n",
       "      <th>shape</th>\n",
       "      <th>shape_instances</th>\n",
       "      <th>image_id</th>\n",
       "      <th>first_command</th>\n",
       "      <th>first_arg</th>\n",
       "      <th>init_container</th>\n",
       "      <th>archive_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98df2866-2131-41c1-97f3-aba6f8761c3d</td>\n",
       "      <td>0LiT6ZNtbpYL</td>\n",
       "      <td>sha256:7ac9e625af2e30671ebec339821489da205116c...</td>\n",
       "      <td>6</td>\n",
       "      <td>sha256:6c05d74eb1fa37a77ed9215d83933265564d661...</td>\n",
       "      <td>N9KxLV2avCo2</td>\n",
       "      <td>BuLIUMMJnyP_</td>\n",
       "      <td>False</td>\n",
       "      <td>archives/compressed/98/98df2866-2131-41c1-97f3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98df2866-2131-41c1-97f3-aba6f8761c3d</td>\n",
       "      <td>0LiT6ZNtbpYL</td>\n",
       "      <td>sha256:b1adc9101829bec6f71530547b1151891a99116...</td>\n",
       "      <td>6</td>\n",
       "      <td>sha256:6c05d74eb1fa37a77ed9215d83933265564d661...</td>\n",
       "      <td>N9KxLV2avCo2</td>\n",
       "      <td>EbplhSJxzSTF</td>\n",
       "      <td>False</td>\n",
       "      <td>archives/compressed/98/98df2866-2131-41c1-97f3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98df2866-2131-41c1-97f3-aba6f8761c3d</td>\n",
       "      <td>0LiT6ZNtbpYL</td>\n",
       "      <td>sha256:b1adc9101829bec6f71530547b1151891a99116...</td>\n",
       "      <td>6</td>\n",
       "      <td>sha256:8b9ecf20324c62d92b4a812a9f502b1059cfed0...</td>\n",
       "      <td>Cl6kTzfbYztA</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>archives/compressed/98/98df2866-2131-41c1-97f3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             cluster_id     namespace  \\\n",
       "0  98df2866-2131-41c1-97f3-aba6f8761c3d  0LiT6ZNtbpYL   \n",
       "1  98df2866-2131-41c1-97f3-aba6f8761c3d  0LiT6ZNtbpYL   \n",
       "2  98df2866-2131-41c1-97f3-aba6f8761c3d  0LiT6ZNtbpYL   \n",
       "\n",
       "                                               shape  shape_instances  \\\n",
       "0  sha256:7ac9e625af2e30671ebec339821489da205116c...                6   \n",
       "1  sha256:b1adc9101829bec6f71530547b1151891a99116...                6   \n",
       "2  sha256:b1adc9101829bec6f71530547b1151891a99116...                6   \n",
       "\n",
       "                                            image_id first_command  \\\n",
       "0  sha256:6c05d74eb1fa37a77ed9215d83933265564d661...  N9KxLV2avCo2   \n",
       "1  sha256:6c05d74eb1fa37a77ed9215d83933265564d661...  N9KxLV2avCo2   \n",
       "2  sha256:8b9ecf20324c62d92b4a812a9f502b1059cfed0...  Cl6kTzfbYztA   \n",
       "\n",
       "      first_arg  init_container  \\\n",
       "0  BuLIUMMJnyP_           False   \n",
       "1  EbplhSJxzSTF           False   \n",
       "2          <NA>            True   \n",
       "\n",
       "                                        archive_path  \n",
       "0  archives/compressed/98/98df2866-2131-41c1-97f3...  \n",
       "1  archives/compressed/98/98df2866-2131-41c1-97f3...  \n",
       "2  archives/compressed/98/98df2866-2131-41c1-97f3...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containers_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942541f-e5f8-4043-a352-e4a3d3213b3c",
   "metadata": {},
   "source": [
    "## Function to extract mapped information for `image_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ccb41ca-31ef-43da-a149-66f6273ac8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_df(image_id):\n",
    "    dataframe = pd.DataFrame([])\n",
    "    base_url = \"https://pyxis.engineering.redhat.com/v1/images?filter=image_id==\"\n",
    "    image_id = image_id\n",
    "    team_url = base_url + str(image_id)\n",
    "    r = requests.get(team_url, auth=kerberos_auth, verify=False)\n",
    "    if r.status_code == 200:\n",
    "        data = json.loads(r.content)\n",
    "        if len(data[\"data\"]) > 0:\n",
    "            if len(data[\"data\"][0][\"parsed_data\"][\"labels\"]) > 0:\n",
    "                df = pd.DataFrame(data[\"data\"][0][\"parsed_data\"][\"labels\"])\n",
    "                table = pd.pivot_table(\n",
    "                    df, values=\"value\", aggfunc=lambda x: x, columns=\"name\"\n",
    "                )\n",
    "                table[\"image_id\"] = image_id\n",
    "                table = table.set_index(\"image_id\")\n",
    "                dataframe = dataframe.append(table)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce809129-2614-4eec-9547-90d05047a22e",
   "metadata": {},
   "source": [
    "## Function to extract mapped information for `image_layer_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38ae437-e1c6-4f71-840e-bd2216c01983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_layer_df(image_id):\n",
    "    dataframe = pd.DataFrame([])\n",
    "    base_url = \"https://pyxis.engineering.redhat.com/v1/images?filter=top_layer_id==\"\n",
    "    team_url = base_url + str(image_id)\n",
    "    r = requests.get(team_url, auth=kerberos_auth, verify=False)\n",
    "    if r.status_code == 200:\n",
    "        data = json.loads(r.content)\n",
    "        if len(data[\"data\"]) > 0:\n",
    "            if len(data[\"data\"][0][\"parsed_data\"][\"labels\"]) > 0:\n",
    "                df = pd.DataFrame(data[\"data\"][0][\"parsed_data\"][\"labels\"])\n",
    "                table = pd.pivot_table(\n",
    "                    df, values=\"value\", aggfunc=lambda x: x, columns=\"name\"\n",
    "                )\n",
    "                table[\"image_id\"] = image_id\n",
    "                table = table.set_index(\"image_id\")\n",
    "                dataframe = dataframe.append(table)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ccbfc-2ca2-4f2a-a15b-a5fb1bfb08f4",
   "metadata": {},
   "source": [
    "## Mapping the SHA's in `image_id` column of Image layers Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fe2a6-205f-4a03-9141-f518e9073238",
   "metadata": {},
   "source": [
    "First, we try to form a list of unique image_id from the image layer dataset. Using that list, we will be doing the web scraping followed by the formation of the dataframe with image_id and corresponding product name, summary, vendor, version, and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f736e6d6-b62a-4d0d-bdf3-74eb01b691ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the list of image_id\n",
    "arr_imageid = image_layers_df.image_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66bf5ec5-494a-4166-a550-0ae32482a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kerberos_auth = HTTPKerberosAuth(mutual_authentication=OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e30a1e6-5cf5-4832-b94b-6ee9c01a4f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5315"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the list\n",
    "len(arr_imageid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa5c1c8-ec71-4803-aeee-8f09d38eaa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# number of max processes\n",
    "n_max_processes = mp.cpu_count()\n",
    "print(n_max_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d73f958-4040-4611-ab21-0df99f1c246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5315/5315 [10:54<00:00,  8.13it/s]\n"
     ]
    }
   ],
   "source": [
    "with mp.Pool(processes=n_max_processes) as pool:\n",
    "    df = list(tqdm(pool.imap(mapped_df, arr_imageid), total=len(arr_imageid)))\n",
    "    dataframe_image_id = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69b03469-ca79-425a-b85c-d46ad66fff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958, 73)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_image_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f95f98-cb5c-43cb-ba1c-0def191902aa",
   "metadata": {},
   "source": [
    "Mapped 956 (~18%) image_id's sha out of 5315 image_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79672f8a-d56c-48e9-a67c-ab2120a54382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Uploading the mapping dataset in the bucket\\nparquet_buffer = io.BytesIO()\\ndataframe_image_id.to_parquet(parquet_buffer)\\ns3_obj = s3.Object(\\n    s3_bucket, \"prototype/workload/image_layers/dataframe_image_id.parquet\"\\n)\\nstatus = s3_obj.put(Body=parquet_buffer.getvalue())\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Uploading the mapping dataset in the bucket\n",
    "parquet_buffer = io.BytesIO()\n",
    "dataframe_image_id.to_parquet(parquet_buffer)\n",
    "s3_obj = s3.Object(\n",
    "    s3_bucket, \"prototype/workload/image_layers/dataframe_image_id.parquet\"\n",
    ")\n",
    "status = s3_obj.put(Body=parquet_buffer.getvalue())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b081e01-0567-4c79-ab56-51559cc7e9cd",
   "metadata": {},
   "source": [
    "The corresponding image_id mapped with the product name is saved in the bucket in the form of dataframe (_dataframe_image_id.parquet_).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799698b-0bde-40c9-a4bb-598fbacc2546",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba9a2e-ec5d-4030-a081-09e9c907c789",
   "metadata": {},
   "source": [
    "## Mapping SHA's from `image_layer_id` column of Image Layers Dataset\n",
    "\n",
    "In addition to the `image_id` column, the `image_layer_id` column also contains image SHA's. These SHA's correspond to the layers that make up the image in `image_id`. In this section, we try to form a list of unique image_layer_id from the image layer dataset. Using that list, we will be doing the web scraping followed by the formation of the dataframe with image_layer_id and corresponding product name, summary, vendor, and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dde40a5-b726-4c16-b622-618e73ff200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_layer_imageid = image_layers_df.layer_image_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1cd911-5747-45fe-80d9-61f13e286263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the list\n",
    "len(arr_layer_imageid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3eafcb7-918a-4a4f-bac2-e13c962e620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17817/17817 [35:21<00:00,  8.40it/s]  \n"
     ]
    }
   ],
   "source": [
    "with mp.Pool(processes=n_max_processes) as pool:\n",
    "    df_image_layerid = list(\n",
    "        tqdm(\n",
    "            pool.imap(mapped_layer_df, arr_layer_imageid),\n",
    "            total=len(arr_layer_imageid),\n",
    "        )\n",
    "    )\n",
    "    df_image_layerid = pd.concat(df_image_layerid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6648fa6d-7647-48b6-82d2-7c7e3bc95f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1292, 105)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image_layerid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1cbea-0052-4366-98c6-8044363e7e49",
   "metadata": {},
   "source": [
    "We were able to create a dataframe which maps 1292 (~7%) sha's of the image layer id provided in the image layer dataset out of 17817 sha's in the image layer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9a8d3be-e7b2-407e-b818-fba8c2a762ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Uploading the mapping dataset in the bucket\\nparquet_buffer = io.BytesIO()\\ndf_image_layer_id.to_parquet(parquet_buffer)\\ns3_obj = s3.Object(\\n    s3_bucket, \"prototype/workload/image_layers/df_image_layer_id.parquet\"\\n)\\nstatus = s3_obj.put(Body=parquet_buffer.getvalue())\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Uploading the mapping dataset in the bucket\n",
    "parquet_buffer = io.BytesIO()\n",
    "df_image_layer_id.to_parquet(parquet_buffer)\n",
    "s3_obj = s3.Object(\n",
    "    s3_bucket, \"prototype/workload/image_layers/df_image_layer_id.parquet\"\n",
    ")\n",
    "status = s3_obj.put(Body=parquet_buffer.getvalue())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3842a0-fb65-48c3-9f1a-ca9cd285be80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82968920-b736-45e3-8569-5fd195a98498",
   "metadata": {},
   "source": [
    "## Mapping the SHA's in`image id` column of Containers dataset\n",
    "\n",
    "In this section, we will map the SHA's in the containers dataset to their product name, summary, vendor, and other attributes. We will first form a list of unique image_id's from the containers dataset. Using that list, we will be doing the web scraping followed by the formation of the dataframe with image_id and corresponding attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1aa7860-e391-4fe8-9370-7c792536d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing out the SHA's of image_id\n",
    "arr_cont_imageid = containers_df.image_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31f34e76-196d-46fa-a08d-2ed281119382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33488"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the list\n",
    "len(arr_cont_imageid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff131d8b-9df6-4357-9968-d573bb1e61e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33488/33488 [1:07:39<00:00,  8.25it/s]\n"
     ]
    }
   ],
   "source": [
    "with mp.Pool(processes=n_max_processes) as pool:\n",
    "    df_cont_image_id = list(\n",
    "        tqdm(pool.imap(mapped_df, arr_cont_imageid), total=len(arr_cont_imageid))\n",
    "    )\n",
    "    df_cont_image_id = pd.concat(df_cont_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32be89c-c85c-49c4-8277-25eb980f2bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8928, 262)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cont_image_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09ab67-9bd2-40e8-b844-403b55e42335",
   "metadata": {},
   "source": [
    "Here, we successfully did the mapping for 8928 (~26%) sha's of the image_id for container dataset out of 33488 sha's of image_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5b58c7-ba1f-4110-8042-5ef9800c4986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Uploading the mapping dataset in the bucket\\nparquet_buffer = io.BytesIO()\\ndf_cont_image_id.to_parquet(parquet_buffer)\\ns3_obj = s3.Object(\\n    s3_bucket, \"prototype/workload/containers/df_cont_image_id.parquet\"\\n)\\nstatus = s3_obj.put(Body=parquet_buffer.getvalue())\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Uploading the mapping dataset in the bucket\n",
    "parquet_buffer = io.BytesIO()\n",
    "df_cont_image_id.to_parquet(parquet_buffer)\n",
    "s3_obj = s3.Object(\n",
    "    s3_bucket, \"prototype/workload/containers/df_cont_image_id.parquet\"\n",
    ")\n",
    "status = s3_obj.put(Body=parquet_buffer.getvalue())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59f876-2c9c-4746-a5b2-2d1df11af1ac",
   "metadata": {},
   "source": [
    "The corresponding mapped dataframe is saved in bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dab56-9028-47b7-9452-c80fdb94a576",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f3949-f8f2-49a8-ab71-b893f1b42b97",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7adea4-e9af-4762-bacd-83c84155201c",
   "metadata": {},
   "source": [
    "The notebook does takes some time to run. In the notebook, we were able to map the product name with the corresponding image_id from the image layer dataset and the container dataset. They mapped dataframe are then saved in the bucket. \n",
    "\n",
    "As next steps, we will be extracting the telemetry information (cpu usage, memory usage) corresponding to different cluster_id in the workload dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
